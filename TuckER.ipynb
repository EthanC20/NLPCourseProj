{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TuckER的pytorch实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.nn.init import xavier_normal_\n",
    "import torch.nn as nn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集和验证集\n",
    "class TripleDataset(data.Dataset):\n",
    "    def __init__(self, ent2id, rel2id, triple_data_list):\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.data = triple_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        head, relation, tail = self.data[index]\n",
    "        head_id = self.ent2id[head]\n",
    "        relation_id = self.rel2id[relation]\n",
    "        tail_id = self.ent2id[tail]\n",
    "        return head_id, relation_id, tail_id\n",
    "\n",
    "# 测试集    \n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, ent2id, rel2id, test_data_list):\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.data = test_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        head, relation = self.data[index]\n",
    "        head_id = self.ent2id[head]\n",
    "        relation_id = self.rel2id[relation]\n",
    "        return head_id, relation_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TuckER模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuckER(nn.Module):\n",
    "    def __init__(self, entity_num, relation_num, dim=100, input_dropout=0.3, hidden_dropout1=0.4, hidden_dropout2=0.5):\n",
    "        super(TuckER, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.entity_num = entity_num\n",
    "\n",
    "        self.E = nn.Embedding(entity_num, dim)\n",
    "        self.R = nn.Embedding(relation_num, dim)\n",
    "        self.W = nn.Parameter(torch.tensor(np.random.uniform(-1, 1, (dim, dim, dim)), \n",
    "                                    dtype=torch.float, device=\"cuda\", requires_grad=True))\n",
    "\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        self.hidden_dropout1 = nn.Dropout(hidden_dropout1)\n",
    "        self.hidden_dropout2 = nn.Dropout(hidden_dropout2)\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(dim)\n",
    "        self.bn1 = nn.BatchNorm1d(dim)\n",
    "\n",
    "    def init(self):\n",
    "        xavier_normal_(self.E.weight.data)\n",
    "        xavier_normal_(self.R.weight.data)\n",
    "\n",
    "    def forward(self, e1_idx, r_idx):\n",
    "        e1 = self.E(e1_idx)\n",
    "        x = self.bn0(e1)\n",
    "        x = self.input_dropout(x)\n",
    "        x = x.view(-1, 1, e1.size(1))\n",
    "\n",
    "        r = self.R(r_idx)\n",
    "        W_mat = torch.mm(r, self.W.view(r.size(1), -1))\n",
    "        W_mat = W_mat.view(-1, e1.size(1), e1.size(1))\n",
    "        W_mat = self.hidden_dropout1(W_mat)\n",
    "\n",
    "        x = torch.bmm(x, W_mat) \n",
    "        x = x.view(-1, e1.size(1))      \n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden_dropout2(x)\n",
    "        x = torch.mm(x, self.E.weight.transpose(1,0))\n",
    "        pred = torch.sigmoid(x)\n",
    "        return pred\n",
    "\n",
    "    def link_predict(self, head, relation, tail=None, k=10):\n",
    "        e1 = self.E(head)\n",
    "        r = self.R(relation)\n",
    "        x = self.bn0(e1)\n",
    "        x = x.view(-1, 1, e1.size(1))\n",
    "        W_mat = torch.mm(r, self.W.view(r.size(1), -1))\n",
    "        W_mat = W_mat.view(-1, e1.size(1), e1.size(1))\n",
    "        W_mat = self.hidden_dropout1(W_mat)\n",
    "        x = torch.bmm(x, W_mat)\n",
    "        x = x.view(-1, e1.size(1))\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden_dropout2(x)\n",
    "        h_add_r = x\n",
    "        scores = torch.mm(h_add_r, self.E.weight.transpose(1, 0))\n",
    "        _, indices = torch.topk(scores, k=k, dim=1, largest=True)\n",
    "        \n",
    "        if tail is not None:\n",
    "            tail = tail.view(-1, 1)\n",
    "            ranks = torch.nonzero(indices == tail).split(1, dim=1)[1] + 1\n",
    "            ranks = torch.where(ranks <= k, ranks.float(), torch.tensor(10000.0, device=ranks.device))\n",
    "            mrr = torch.sum(1 / ranks)\n",
    "            hits_1_num = torch.sum(indices[:, :1] == tail).item()\n",
    "            hits_3_num = torch.sum(indices[:, :3] == tail).item()\n",
    "            hits_10_num = torch.sum(indices[:, :10] == tail).item()\n",
    "            return mrr, hits_1_num, hits_3_num, hits_10_num\n",
    "        \n",
    "        return indices[:, :k]\n",
    "\n",
    "    def evaluate(self, data_loader):\n",
    "        mrr_sum = hits_1_nums = hits_3_nums = hits_10_nums = 0\n",
    "        total_samples = 0\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for heads, relations, tails in tqdm.tqdm(data_loader):\n",
    "                mrr_sum_batch, hits_1_num, hits_3_num, hits_10_num = self.link_predict(heads.to(device), relations.to(device), tails.to(device))\n",
    "                mrr_sum += mrr_sum_batch.item()\n",
    "                hits_1_nums += hits_1_num\n",
    "                hits_3_nums += hits_3_num\n",
    "                hits_10_nums += hits_10_num\n",
    "                total_samples += len(heads)\n",
    "        \n",
    "        return mrr_sum / total_samples, hits_1_nums / total_samples, hits_3_nums / total_samples, hits_10_nums / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchsize增大，得分略有上升\n",
    "train_batch_size = 128\n",
    "dev_batch_size = 16  # 显存不够就调小\n",
    "test_batch_size = 16\n",
    "epochs = 10\n",
    "print_frequency = 5  # 每多少step输出一次信息\n",
    "validation = True  # 是否验证，验证比较费时\n",
    "dev_interval = 5  # 每多少轮验证一次，微调设小一点，会保存最佳权重\n",
    "best_mrr = 0\n",
    "learning_rate = 0.0005  # 学习率建议粗调0.01-0.001，精调0.001-0.0001\n",
    "decay_rate = 0.  # 学习率衰减率\n",
    "label_smoothing = 0.1  # 标签平滑，0.1效果不错\n",
    "embedding_dim = 100  # 维度增大可能会有提升，我感觉没用，100维包含的信息足够丰富"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OpenBG500/OpenBG500_entity2text.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    lines = [line.strip('\\n').split('\\t') for line in dat]\n",
    "ent2id = {line[0]: i for i, line in enumerate(lines)}\n",
    "id2ent = {i: line[0] for i, line in enumerate(lines)}\n",
    "with open('OpenBG500/OpenBG500_relation2text.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    lines = [line.strip().split('\\t') for line in dat]\n",
    "rel2id = {line[0]: i for i, line in enumerate(lines)}\n",
    "with open('OpenBG500/OpenBG500_train.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    train = [line.strip('\\n').split('\\t') for line in dat]\n",
    "with open('OpenBG500/OpenBG500_dev.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    dev = [line.strip('\\n').split('\\t') for line in dat]\n",
    "with open('OpenBG500/OpenBG500_test.tsv', 'r', encoding='utf-8') as fp:\n",
    "    test = fp.readlines()\n",
    "    test = [line.strip('\\n').split('\\t') for line in test]\n",
    "# 构建数据集\n",
    "train_dataset = TripleDataset(ent2id, rel2id, train)\n",
    "dev_dataset = TripleDataset(ent2id, rel2id, dev)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "dev_data_loader = data.DataLoader(dev_dataset, batch_size=dev_batch_size)\n",
    "test_dataset = TestDataset(ent2id, rel2id, test)\n",
    "test_data_loader = data.DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# 获取原始数据集的长度\n",
    "original_size = len(train_dataset)\n",
    "\n",
    "# 计算切片后的目标大小\n",
    "target_size = original_size // 100\n",
    "\n",
    "# 随机抽样得到切片后的索引\n",
    "sampled_indices = random.sample(range(original_size), target_size)\n",
    "\n",
    "# 根据抽样后的索引获取切片后的数据集\n",
    "sampled_train_data = [train_dataset[idx] for idx in sampled_indices]\n",
    "\n",
    "# 如果需要将切片后的数据集重新构建为 DataLoader\n",
    "sampled_train_data_loader = data.DataLoader(sampled_train_data, batch_size=train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\NLP\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch:0/10, step:0/98, loss=0.6933451294898987, avg_loss=0.6933451294898987\n",
      "epoch:0/10, step:5/98, loss=0.6944721937179565, avg_loss=0.6933950086434683\n",
      "epoch:0/10, step:10/98, loss=0.6920927166938782, avg_loss=0.6929111318154768\n",
      "epoch:0/10, step:15/98, loss=0.6930013298988342, avg_loss=0.6929364651441574\n",
      "epoch:0/10, step:20/98, loss=0.6907093524932861, avg_loss=0.6930092743464878\n",
      "epoch:0/10, step:25/98, loss=0.6927011013031006, avg_loss=0.6928263283692874\n",
      "epoch:0/10, step:30/98, loss=0.6927152872085571, avg_loss=0.6926916703101127\n",
      "epoch:0/10, step:35/98, loss=0.6923795342445374, avg_loss=0.692551831404368\n",
      "epoch:0/10, step:40/98, loss=0.6915583610534668, avg_loss=0.6923728759695844\n",
      "epoch:0/10, step:45/98, loss=0.6888608932495117, avg_loss=0.6920819114083829\n",
      "epoch:0/10, step:50/98, loss=0.6866605281829834, avg_loss=0.6919017387371437\n",
      "epoch:0/10, step:55/98, loss=0.6874369382858276, avg_loss=0.6916940233537129\n",
      "epoch:0/10, step:60/98, loss=0.6891454458236694, avg_loss=0.6913722225877105\n",
      "epoch:0/10, step:65/98, loss=0.6855908632278442, avg_loss=0.6909180572538665\n",
      "epoch:0/10, step:70/98, loss=0.6856174468994141, avg_loss=0.69049587719877\n",
      "epoch:0/10, step:75/98, loss=0.6829348802566528, avg_loss=0.6899409850961283\n",
      "epoch:0/10, step:80/98, loss=0.6760167479515076, avg_loss=0.6892078776418427\n",
      "epoch:0/10, step:85/98, loss=0.6727664470672607, avg_loss=0.6885102515996888\n",
      "epoch:0/10, step:90/98, loss=0.6629210114479065, avg_loss=0.6876480618675986\n",
      "epoch:0/10, step:95/98, loss=0.669558048248291, avg_loss=0.6865964786460003\n",
      "epoch:0/10, all_loss=67.25880336761475\n",
      "epoch:1/10, step:0/98, loss=0.6662802696228027, avg_loss=0.6662802696228027\n",
      "epoch:1/10, step:5/98, loss=0.6607726216316223, avg_loss=0.6653541624546051\n",
      "epoch:1/10, step:10/98, loss=0.6556515693664551, avg_loss=0.6627067110755227\n",
      "epoch:1/10, step:15/98, loss=0.6377518177032471, avg_loss=0.6594225689768791\n",
      "epoch:1/10, step:20/98, loss=0.6452339291572571, avg_loss=0.65694526831309\n",
      "epoch:1/10, step:25/98, loss=0.6374377608299255, avg_loss=0.6537410295926608\n",
      "epoch:1/10, step:30/98, loss=0.6408252120018005, avg_loss=0.6518550899720961\n",
      "epoch:1/10, step:35/98, loss=0.6180883049964905, avg_loss=0.6481137176354727\n",
      "epoch:1/10, step:40/98, loss=0.6154484152793884, avg_loss=0.6439600339749965\n",
      "epoch:1/10, step:45/98, loss=0.6077743768692017, avg_loss=0.6404441983803458\n",
      "epoch:1/10, step:50/98, loss=0.591355562210083, avg_loss=0.6360750771036335\n",
      "epoch:1/10, step:55/98, loss=0.5846760272979736, avg_loss=0.6314403255070958\n",
      "epoch:1/10, step:60/98, loss=0.586821973323822, avg_loss=0.6272254078114619\n",
      "epoch:1/10, step:65/98, loss=0.5873832702636719, avg_loss=0.6234400597485629\n",
      "epoch:1/10, step:70/98, loss=0.5592209696769714, avg_loss=0.6188956306014263\n",
      "epoch:1/10, step:75/98, loss=0.5401172041893005, avg_loss=0.6144183458466279\n",
      "epoch:1/10, step:80/98, loss=0.5495191812515259, avg_loss=0.6099419218522532\n",
      "epoch:1/10, step:85/98, loss=0.5148622393608093, avg_loss=0.6049265092195466\n",
      "epoch:1/10, step:90/98, loss=0.5229601263999939, avg_loss=0.599926179254448\n",
      "epoch:1/10, step:95/98, loss=0.4999960660934448, avg_loss=0.5948881078511477\n",
      "epoch:1/10, all_loss=58.09864729642868\n",
      "epoch:2/10, step:0/98, loss=0.4798559248447418, avg_loss=0.4798559248447418\n",
      "epoch:2/10, step:5/98, loss=0.4462355673313141, avg_loss=0.4761342902978261\n",
      "epoch:2/10, step:10/98, loss=0.45667943358421326, avg_loss=0.47026488997719507\n",
      "epoch:2/10, step:15/98, loss=0.43915513157844543, avg_loss=0.4616041462868452\n",
      "epoch:2/10, step:20/98, loss=0.4322254955768585, avg_loss=0.458438305627732\n",
      "epoch:2/10, step:25/98, loss=0.41494452953338623, avg_loss=0.4524733298099958\n",
      "epoch:2/10, step:30/98, loss=0.40499553084373474, avg_loss=0.44610995727200664\n",
      "epoch:2/10, step:35/98, loss=0.41808754205703735, avg_loss=0.44166655507352615\n",
      "epoch:2/10, step:40/98, loss=0.3683580160140991, avg_loss=0.435132499148206\n",
      "epoch:2/10, step:45/98, loss=0.39114055037498474, avg_loss=0.4295534018589103\n",
      "epoch:2/10, step:50/98, loss=0.3841427266597748, avg_loss=0.4234188491222905\n",
      "epoch:2/10, step:55/98, loss=0.35332542657852173, avg_loss=0.4166142466877188\n",
      "epoch:2/10, step:60/98, loss=0.3464891314506531, avg_loss=0.41083830106453817\n",
      "epoch:2/10, step:65/98, loss=0.35612186789512634, avg_loss=0.4055236072251291\n",
      "epoch:2/10, step:70/98, loss=0.3072375953197479, avg_loss=0.3993793851892713\n",
      "epoch:2/10, step:75/98, loss=0.28973519802093506, avg_loss=0.3934175446629524\n",
      "epoch:2/10, step:80/98, loss=0.29314854741096497, avg_loss=0.3879011700182785\n",
      "epoch:2/10, step:85/98, loss=0.28000059723854065, avg_loss=0.3819590907457263\n",
      "epoch:2/10, step:90/98, loss=0.2847006916999817, avg_loss=0.3760221934580541\n",
      "epoch:2/10, step:95/98, loss=0.26315736770629883, avg_loss=0.3700362734186153\n",
      "epoch:2/10, all_loss=36.09019477665424\n",
      "epoch:3/10, step:0/98, loss=0.2919720709323883, avg_loss=0.2919720709323883\n",
      "epoch:3/10, step:5/98, loss=0.24844710528850555, avg_loss=0.26566775391499203\n",
      "epoch:3/10, step:10/98, loss=0.22187159955501556, avg_loss=0.25640332427891815\n",
      "epoch:3/10, step:15/98, loss=0.23852680623531342, avg_loss=0.2498506773263216\n",
      "epoch:3/10, step:20/98, loss=0.23406293988227844, avg_loss=0.24498478713489713\n",
      "epoch:3/10, step:25/98, loss=0.2098807394504547, avg_loss=0.24106235343676347\n",
      "epoch:3/10, step:30/98, loss=0.22398976981639862, avg_loss=0.2376749635704102\n",
      "epoch:3/10, step:35/98, loss=0.23411422967910767, avg_loss=0.23325216066506174\n",
      "epoch:3/10, step:40/98, loss=0.1713397204875946, avg_loss=0.2296971998563627\n",
      "epoch:3/10, step:45/98, loss=0.2051416039466858, avg_loss=0.2269912329704865\n",
      "epoch:3/10, step:50/98, loss=0.18218478560447693, avg_loss=0.22335503878546695\n",
      "epoch:3/10, step:55/98, loss=0.20170001685619354, avg_loss=0.2205073633896453\n",
      "epoch:3/10, step:60/98, loss=0.16745491325855255, avg_loss=0.2171874512903026\n",
      "epoch:3/10, step:65/98, loss=0.14749550819396973, avg_loss=0.21380658041347156\n",
      "epoch:3/10, step:70/98, loss=0.1646771878004074, avg_loss=0.21084346317909133\n",
      "epoch:3/10, step:75/98, loss=0.15733657777309418, avg_loss=0.20761954372650698\n",
      "epoch:3/10, step:80/98, loss=0.16141562163829803, avg_loss=0.20488967571729494\n",
      "epoch:3/10, step:85/98, loss=0.16608896851539612, avg_loss=0.20244660852260368\n",
      "epoch:3/10, step:90/98, loss=0.1652000993490219, avg_loss=0.19988412188959645\n",
      "epoch:3/10, step:95/98, loss=0.1366846263408661, avg_loss=0.1968644472459952\n",
      "epoch:3/10, all_loss=19.165563747286797\n",
      "epoch:4/10, step:0/98, loss=0.1431349217891693, avg_loss=0.1431349217891693\n",
      "epoch:4/10, step:5/98, loss=0.12074705213308334, avg_loss=0.13407654936114946\n",
      "epoch:4/10, step:10/98, loss=0.1216244027018547, avg_loss=0.13489707627079703\n",
      "epoch:4/10, step:15/98, loss=0.13418705761432648, avg_loss=0.13402380794286728\n",
      "epoch:4/10, step:20/98, loss=0.12546418607234955, avg_loss=0.13302401169424966\n",
      "epoch:4/10, step:25/98, loss=0.14986810088157654, avg_loss=0.13286604187809503\n",
      "epoch:4/10, step:30/98, loss=0.11544156819581985, avg_loss=0.13049403001223842\n",
      "epoch:4/10, step:35/98, loss=0.11322957277297974, avg_loss=0.1291949430273639\n",
      "epoch:4/10, step:40/98, loss=0.12914124131202698, avg_loss=0.12758939022698054\n",
      "epoch:4/10, step:45/98, loss=0.09440689533948898, avg_loss=0.12572213715833166\n",
      "epoch:4/10, step:50/98, loss=0.10231418907642365, avg_loss=0.12424367242584042\n",
      "epoch:4/10, step:55/98, loss=0.09927932918071747, avg_loss=0.1225366909056902\n",
      "epoch:4/10, step:60/98, loss=0.09699610620737076, avg_loss=0.12104839880446919\n",
      "epoch:4/10, step:65/98, loss=0.09884518384933472, avg_loss=0.11995472092971657\n",
      "epoch:4/10, step:70/98, loss=0.09152612835168839, avg_loss=0.11895509305554376\n",
      "epoch:4/10, step:75/98, loss=0.09927387535572052, avg_loss=0.11816872136765405\n",
      "epoch:4/10, step:80/98, loss=0.08954306691884995, avg_loss=0.11655526433461978\n",
      "epoch:4/10, step:85/98, loss=0.08619026094675064, avg_loss=0.11520308126197305\n",
      "epoch:4/10, step:90/98, loss=0.09637878835201263, avg_loss=0.11399757026971041\n",
      "epoch:4/10, step:95/98, loss=0.09385508298873901, avg_loss=0.11303381451095144\n",
      "epoch:4/10, all_loss=11.029395550489426\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:00<00:00, 395.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.0, hit@1: 0.0, hit@3: 0.0, hit@10: 0.0  *\n",
      "epoch:5/10, step:0/98, loss=0.07695478945970535, avg_loss=0.07695478945970535\n",
      "epoch:5/10, step:5/98, loss=0.09578299522399902, avg_loss=0.08556129038333893\n",
      "epoch:5/10, step:10/98, loss=0.08213818818330765, avg_loss=0.08610222014513882\n",
      "epoch:5/10, step:15/98, loss=0.07722155749797821, avg_loss=0.08573900582268834\n",
      "epoch:5/10, step:20/98, loss=0.0872800350189209, avg_loss=0.08546199685051328\n",
      "epoch:5/10, step:25/98, loss=0.07610221952199936, avg_loss=0.08490246820908326\n",
      "epoch:5/10, step:30/98, loss=0.07478482276201248, avg_loss=0.0845721306339387\n",
      "epoch:5/10, step:35/98, loss=0.07482743263244629, avg_loss=0.08360771131184366\n",
      "epoch:5/10, step:40/98, loss=0.06941603124141693, avg_loss=0.08225804749058514\n",
      "epoch:5/10, step:45/98, loss=0.07637420296669006, avg_loss=0.0815462628784387\n",
      "epoch:5/10, step:50/98, loss=0.0688987523317337, avg_loss=0.08032348693585863\n",
      "epoch:5/10, step:55/98, loss=0.07197542488574982, avg_loss=0.0795723001605698\n",
      "epoch:5/10, step:60/98, loss=0.07863841950893402, avg_loss=0.07886036625895344\n",
      "epoch:5/10, step:65/98, loss=0.0749613493680954, avg_loss=0.07839572582055222\n",
      "epoch:5/10, step:70/98, loss=0.07427436858415604, avg_loss=0.0780038138405538\n",
      "epoch:5/10, step:75/98, loss=0.0651741772890091, avg_loss=0.07722276223725394\n",
      "epoch:5/10, step:80/98, loss=0.07576059550046921, avg_loss=0.07648321231942118\n",
      "epoch:5/10, step:85/98, loss=0.05856681615114212, avg_loss=0.07579983311683633\n",
      "epoch:5/10, step:90/98, loss=0.05634532496333122, avg_loss=0.07528095368991841\n",
      "epoch:5/10, step:95/98, loss=0.07642365992069244, avg_loss=0.07468981602384399\n",
      "epoch:5/10, all_loss=7.296123970299959\n",
      "epoch:6/10, step:0/98, loss=0.05824820324778557, avg_loss=0.05824820324778557\n",
      "epoch:6/10, step:5/98, loss=0.06541317701339722, avg_loss=0.06373884094258149\n",
      "epoch:6/10, step:10/98, loss=0.06044236198067665, avg_loss=0.06335477327758615\n",
      "epoch:6/10, step:15/98, loss=0.0693613812327385, avg_loss=0.06432095961645246\n",
      "epoch:6/10, step:20/98, loss=0.06251256912946701, avg_loss=0.06399631322849364\n",
      "epoch:6/10, step:25/98, loss=0.05504516139626503, avg_loss=0.06269798384836087\n",
      "epoch:6/10, step:30/98, loss=0.06307891011238098, avg_loss=0.06194705059451441\n",
      "epoch:6/10, step:35/98, loss=0.06041719764471054, avg_loss=0.06138345506042242\n",
      "epoch:6/10, step:40/98, loss=0.057774003595113754, avg_loss=0.06067904366589174\n",
      "epoch:6/10, step:45/98, loss=0.0472659133374691, avg_loss=0.05991504713892937\n",
      "epoch:6/10, step:50/98, loss=0.05804819613695145, avg_loss=0.0600245944863441\n",
      "epoch:6/10, step:55/98, loss=0.05823086202144623, avg_loss=0.06015645592872586\n",
      "epoch:6/10, step:60/98, loss=0.0503099225461483, avg_loss=0.05974689822216503\n",
      "epoch:6/10, step:65/98, loss=0.05687825009226799, avg_loss=0.05937972399547244\n",
      "epoch:6/10, step:70/98, loss=0.04956181347370148, avg_loss=0.05888709126853607\n",
      "epoch:6/10, step:75/98, loss=0.045514922589063644, avg_loss=0.05868422396873173\n",
      "epoch:6/10, step:80/98, loss=0.05740642920136452, avg_loss=0.058327586838492644\n",
      "epoch:6/10, step:85/98, loss=0.04656355082988739, avg_loss=0.058058205602127454\n",
      "epoch:6/10, step:90/98, loss=0.04801623895764351, avg_loss=0.05792399143779671\n",
      "epoch:6/10, step:95/98, loss=0.05462885648012161, avg_loss=0.05758634915885826\n",
      "epoch:6/10, all_loss=5.626041393727064\n",
      "epoch:7/10, step:0/98, loss=0.05274398624897003, avg_loss=0.05274398624897003\n",
      "epoch:7/10, step:5/98, loss=0.05936930328607559, avg_loss=0.053048957139253616\n",
      "epoch:7/10, step:10/98, loss=0.0507381409406662, avg_loss=0.05212980305606669\n",
      "epoch:7/10, step:15/98, loss=0.04484930261969566, avg_loss=0.050811694003641605\n",
      "epoch:7/10, step:20/98, loss=0.048917949199676514, avg_loss=0.05154405676183246\n",
      "epoch:7/10, step:25/98, loss=0.045751605182886124, avg_loss=0.05063464163014522\n",
      "epoch:7/10, step:30/98, loss=0.04619106277823448, avg_loss=0.050378738031271963\n",
      "epoch:7/10, step:35/98, loss=0.0518435537815094, avg_loss=0.05019562805278434\n",
      "epoch:7/10, step:40/98, loss=0.04179961234331131, avg_loss=0.04966785331688276\n",
      "epoch:7/10, step:45/98, loss=0.04558485746383667, avg_loss=0.04964688679446345\n",
      "epoch:7/10, step:50/98, loss=0.044353023171424866, avg_loss=0.04937665266733544\n",
      "epoch:7/10, step:55/98, loss=0.0455048531293869, avg_loss=0.04917511363912906\n",
      "epoch:7/10, step:60/98, loss=0.04139895364642143, avg_loss=0.0489156852613707\n",
      "epoch:7/10, step:65/98, loss=0.046209461987018585, avg_loss=0.04862015874999942\n",
      "epoch:7/10, step:70/98, loss=0.05045487359166145, avg_loss=0.048310744300694534\n",
      "epoch:7/10, step:75/98, loss=0.04303513094782829, avg_loss=0.04807988402286643\n",
      "epoch:7/10, step:80/98, loss=0.04580782726407051, avg_loss=0.04791951680808892\n",
      "epoch:7/10, step:85/98, loss=0.044128622859716415, avg_loss=0.04770744756557221\n",
      "epoch:7/10, step:90/98, loss=0.05171108618378639, avg_loss=0.04750993475317955\n",
      "epoch:7/10, step:95/98, loss=0.041285961866378784, avg_loss=0.04736741480883211\n",
      "epoch:7/10, all_loss=4.647990293800831\n",
      "epoch:8/10, step:0/98, loss=0.04188324138522148, avg_loss=0.04188324138522148\n",
      "epoch:8/10, step:5/98, loss=0.04759139195084572, avg_loss=0.04567228568096956\n",
      "epoch:8/10, step:10/98, loss=0.039700478315353394, avg_loss=0.045221628113226456\n",
      "epoch:8/10, step:15/98, loss=0.04731104150414467, avg_loss=0.044303820701316\n",
      "epoch:8/10, step:20/98, loss=0.04094157740473747, avg_loss=0.043969809831607906\n",
      "epoch:8/10, step:25/98, loss=0.04425153136253357, avg_loss=0.043478804712112136\n",
      "epoch:8/10, step:30/98, loss=0.047054681926965714, avg_loss=0.04349037788568005\n",
      "epoch:8/10, step:35/98, loss=0.05215888470411301, avg_loss=0.043723690427011914\n",
      "epoch:8/10, step:40/98, loss=0.039886943995952606, avg_loss=0.04341589532247404\n",
      "epoch:8/10, step:45/98, loss=0.04004448279738426, avg_loss=0.0431990915828425\n",
      "epoch:8/10, step:50/98, loss=0.04449125751852989, avg_loss=0.04329084247058513\n",
      "epoch:8/10, step:55/98, loss=0.04215890169143677, avg_loss=0.04291374209736075\n",
      "epoch:8/10, step:60/98, loss=0.0374559611082077, avg_loss=0.042702023855975414\n",
      "epoch:8/10, step:65/98, loss=0.03989126905798912, avg_loss=0.04258305173028599\n",
      "epoch:8/10, step:70/98, loss=0.03785042464733124, avg_loss=0.04245049333278562\n",
      "epoch:8/10, step:75/98, loss=0.038253773003816605, avg_loss=0.04226988544197459\n",
      "epoch:8/10, step:80/98, loss=0.040970634669065475, avg_loss=0.042192729075013855\n",
      "epoch:8/10, step:85/98, loss=0.047838691622018814, avg_loss=0.04213119519138059\n",
      "epoch:8/10, step:90/98, loss=0.037032756954431534, avg_loss=0.04203830986887544\n",
      "epoch:8/10, step:95/98, loss=0.04241633787751198, avg_loss=0.04199386516120285\n",
      "epoch:8/10, all_loss=4.11550672352314\n",
      "epoch:9/10, step:0/98, loss=0.038107044994831085, avg_loss=0.038107044994831085\n",
      "epoch:9/10, step:5/98, loss=0.03859947249293327, avg_loss=0.0402900247524182\n",
      "epoch:9/10, step:10/98, loss=0.04068823903799057, avg_loss=0.03947474705902013\n",
      "epoch:9/10, step:15/98, loss=0.037308819591999054, avg_loss=0.039231240982189775\n",
      "epoch:9/10, step:20/98, loss=0.03846191614866257, avg_loss=0.038953241670415514\n",
      "epoch:9/10, step:25/98, loss=0.04420558363199234, avg_loss=0.03877274219233256\n",
      "epoch:9/10, step:30/98, loss=0.03795868903398514, avg_loss=0.03879750532007987\n",
      "epoch:9/10, step:35/98, loss=0.03565407544374466, avg_loss=0.038789302214152284\n",
      "epoch:9/10, step:40/98, loss=0.035775236785411835, avg_loss=0.038627552277431254\n",
      "epoch:9/10, step:45/98, loss=0.04241200536489487, avg_loss=0.03845552031112754\n",
      "epoch:9/10, step:50/98, loss=0.0370345413684845, avg_loss=0.03836235805761581\n",
      "epoch:9/10, step:55/98, loss=0.041081324219703674, avg_loss=0.03841714068715062\n",
      "epoch:9/10, step:60/98, loss=0.0360427126288414, avg_loss=0.038395684762079205\n",
      "epoch:9/10, step:65/98, loss=0.04489285871386528, avg_loss=0.03838321448049762\n",
      "epoch:9/10, step:70/98, loss=0.03728131577372551, avg_loss=0.03825818616109834\n",
      "epoch:9/10, step:75/98, loss=0.03357983008027077, avg_loss=0.038152532406935565\n",
      "epoch:9/10, step:80/98, loss=0.03407374024391174, avg_loss=0.038217264974926723\n",
      "epoch:9/10, step:85/98, loss=0.035895295441150665, avg_loss=0.038147950198414715\n",
      "epoch:9/10, step:90/98, loss=0.03350059315562248, avg_loss=0.03799414233519481\n",
      "epoch:9/10, step:95/98, loss=0.034057557582855225, avg_loss=0.037873571583380304\n",
      "epoch:9/10, all_loss=3.707497287541628\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:00<00:00, 422.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 0.0, hit@1: 0.0, hit@3: 0.0, hit@10: 0.0  *\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = TuckER(len(ent2id), len(rel2id), dim=embedding_dim).cuda()\n",
    "model.init()\n",
    "# model.load_state_dict(torch.load('TuckER_best.pth'))\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练\n",
    "print('start training...')\n",
    "for epoch in range(epochs):\n",
    "    all_loss = 0\n",
    "    for i, (local_heads, local_relations, local_tails) in enumerate(sampled_train_data_loader):\n",
    "\n",
    "        head = local_heads.cuda()\n",
    "        relation = local_relations.cuda()\n",
    "        tail = local_tails.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 计算正样本预测值\n",
    "        pred = model.forward(head, relation)\n",
    "        labels = torch.ones(len(local_heads), len(ent2id)).cuda()\n",
    "\n",
    "        # 计算损失\n",
    "        loss = model.loss(pred, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        all_loss += loss.item()\n",
    "        if i % print_frequency == 0:\n",
    "            print(\n",
    "                f\"epoch:{epoch}/{epochs}, step:{i}/{len(sampled_train_data_loader)}, loss={loss.item()}, avg_loss={all_loss / (i + 1)}\")\n",
    "    print(f\"epoch:{epoch}/{epochs}, all_loss={all_loss}\")\n",
    "\n",
    "    # 验证\n",
    "    if validation and (epoch + 1) % dev_interval == 0:\n",
    "        print('testing...')\n",
    "        improve = ''\n",
    "        mrr, hits1, hits3, hits10 = model.evaluate(dev_data_loader)\n",
    "        if mrr >= best_mrr:\n",
    "            best_mrr = mrr\n",
    "            improve = '*'\n",
    "            torch.save(model.state_dict(), 'TuckER_best.pth')\n",
    "        torch.save(model.state_dict(), 'TuckER_latest.pth')\n",
    "        print(f'mrr: {mrr}, hit@1: {hits1}, hit@3: {hits3}, hit@10: {hits10}  {improve}')\n",
    "    if not validation:\n",
    "        torch.save(model.state_dict(), 'TuckER_latest.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_all = []\n",
    "model = TuckER(len(ent2id), len(rel2id), dim=embedding_dim).cuda()\n",
    "model.load_state_dict(torch.load('TuckER_best.pth'))\n",
    "for heads, relations in tqdm.tqdm(test_data_loader):\n",
    "    # 预测的id,结果为tensor(batch_size*10)\n",
    "    predict_id = model.link_predict(heads.cuda(), relations.cuda())\n",
    "    # 结果取到cpu并转为一行的list以便迭代\n",
    "    predict_list = predict_id.cpu().numpy().reshape(1,-1).squeeze(0).tolist()\n",
    "    # id转为实体\n",
    "    predict_ent = map(lambda x: id2ent[x], predict_list)\n",
    "    # 保存结果\n",
    "    predict_all.extend(predict_ent)\n",
    "print('prediction finished !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TuckER_submission.tsv', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test)):\n",
    "        # 直接writelines没有空格分隔，手工加分割符，得按提交格式来\n",
    "        list = [x + '\\t' for x in test[i]] + [x + '\\n' if i == 9 else x + '\\t' for i, x in enumerate(predict_all[i*10:i*10+10])]\n",
    "        f.writelines(list)\n",
    "print('file saved !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
