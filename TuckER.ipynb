{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TuckER的pytorch实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch.nn.init import xavier_normal_\n",
    "import torch.nn as nn\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据集\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练集和验证集\n",
    "class TripleDataset(data.Dataset):\n",
    "    def __init__(self, ent2id, rel2id, triple_data_list):\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.data = triple_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        head, relation, tail = self.data[index]\n",
    "        head_id = self.ent2id[head]\n",
    "        relation_id = self.rel2id[relation]\n",
    "        tail_id = self.ent2id[tail]\n",
    "        return head_id, relation_id, tail_id\n",
    "\n",
    "# 测试集    \n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, ent2id, rel2id, test_data_list):\n",
    "        self.ent2id = ent2id\n",
    "        self.rel2id = rel2id\n",
    "        self.data = test_data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        head, relation = self.data[index]\n",
    "        head_id = self.ent2id[head]\n",
    "        relation_id = self.rel2id[relation]\n",
    "        return head_id, relation_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TuckER模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TuckER(nn.Module):\n",
    "    def __init__(self, entity_num, relation_num, dim=100, input_dropout=0.3, hidden_dropout1=0.4, hidden_dropout2=0.5):\n",
    "        # d\n",
    "        super(TuckER, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.entity_num = entity_num\n",
    "\n",
    "        self.E = nn.Embedding(entity_num, dim)\n",
    "        self.R = nn.Embedding(relation_num, dim)\n",
    "        self.W = nn.Parameter(torch.tensor(np.random.uniform(-1, 1, (dim, dim, dim)), \n",
    "                                    dtype=torch.float, device=\"cuda\", requires_grad=True))\n",
    "\n",
    "        self.input_dropout = nn.Dropout(input_dropout)\n",
    "        self.hidden_dropout1 = nn.Dropout(hidden_dropout1)\n",
    "        self.hidden_dropout2 = nn.Dropout(hidden_dropout2)\n",
    "        self.loss = nn.BCELoss()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm1d(dim)\n",
    "        self.bn1 = nn.BatchNorm1d(dim)\n",
    "        \n",
    "\n",
    "    def init(self):\n",
    "        xavier_normal_(self.E.weight.data)\n",
    "        xavier_normal_(self.R.weight.data)\n",
    "\n",
    "    def forward(self, e1_idx, r_idx):\n",
    "        e1 = self.E(e1_idx)\n",
    "        x = self.bn0(e1)\n",
    "        x = self.input_dropout(x)\n",
    "        x = x.view(-1, 1, e1.size(1))\n",
    "\n",
    "        r = self.R(r_idx)\n",
    "        W_mat = torch.mm(r, self.W.view(r.size(1), -1))\n",
    "        W_mat = W_mat.view(-1, e1.size(1), e1.size(1))\n",
    "        W_mat = self.hidden_dropout1(W_mat)\n",
    "\n",
    "        x = torch.bmm(x, W_mat) \n",
    "        x = x.view(-1, e1.size(1))      \n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden_dropout2(x)\n",
    "        x = torch.mm(x, self.E.weight.transpose(1,0))\n",
    "        pred = torch.sigmoid(x)\n",
    "        return pred\n",
    "\n",
    "    def link_predict(self, head, relation, tail=None, k=10):\n",
    "        e1 = self.E(head)\n",
    "        r = self.R(relation)\n",
    "        x = self.bn0(e1)\n",
    "        x = x.view(-1, 1, e1.size(1))\n",
    "        W_mat = torch.mm(r, self.W.view(r.size(1), -1))\n",
    "        W_mat = W_mat.view(-1, e1.size(1), e1.size(1))\n",
    "        W_mat = self.hidden_dropout1(W_mat)\n",
    "        x = torch.bmm(x, W_mat)\n",
    "        x = x.view(-1, e1.size(1))\n",
    "        x = self.bn1(x)\n",
    "        x = self.hidden_dropout2(x)\n",
    "        h_add_r = x\n",
    "        scores = torch.mm(h_add_r, self.E.weight.transpose(1, 0))\n",
    "        _, indices = torch.topk(scores, k=k, dim=1, largest=True)\n",
    "        \n",
    "        if tail is not None:\n",
    "            tail = tail.view(-1, 1)\n",
    "            rank_num = torch.eq(indices, tail).nonzero().permute(1, 0)[1] + 1\n",
    "            rank_num[rank_num > 9] = 10000\n",
    "            mrr = torch.sum(1 / rank_num.float())\n",
    "            hits_1_num = torch.sum(torch.eq(indices[:, :1], tail)).item()\n",
    "            hits_3_num = torch.sum(torch.eq(indices[:, :3], tail)).item()\n",
    "            hits_10_num = torch.sum(torch.eq(indices[:, :10], tail)).item()\n",
    "            return mrr, hits_1_num, hits_3_num, hits_10_num\n",
    "        \n",
    "        return indices[:, :k]\n",
    "\n",
    "\n",
    "    def evaluate(self, data_loader, dev_num=5000.0):\n",
    "        mrr_sum = hits_1_nums = hits_3_nums = hits_10_nums = 0\n",
    "        device = next(self.parameters()).device\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for heads, relations, tails in tqdm.tqdm(data_loader):\n",
    "                mrr_sum_batch, hits_1_num, hits_3_num, hits_10_num = self.link_predict(heads.to(device), relations.to(device), tails.to(device))\n",
    "                mrr_sum += mrr_sum_batch\n",
    "                hits_1_nums += hits_1_num\n",
    "                hits_3_nums += hits_3_num\n",
    "                hits_10_nums += hits_10_num\n",
    "        \n",
    "        return mrr_sum / dev_num, hits_1_nums / dev_num, hits_3_nums / dev_num, hits_10_nums / dev_num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchsize增大，得分略有上升\n",
    "train_batch_size = 1000\n",
    "dev_batch_size = 16  # 显存不够就调小\n",
    "test_batch_size = 16\n",
    "epochs = 10\n",
    "print_frequency = 5  # 每多少step输出一次信息\n",
    "validation = True  # 是否验证，验证比较费时\n",
    "dev_interval = 5  # 每多少轮验证一次，微调设小一点，会保存最佳权重\n",
    "best_mrr = 0\n",
    "learning_rate = 0.0005  # 学习率建议粗调0.01-0.001，精调0.001-0.0001\n",
    "embedding_dim = 100  # 维度增大可能会有提升，我感觉没用，100维包含的信息足够丰富"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('OpenBG500/OpenBG500_entity2text.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    lines = [line.strip('\\n').split('\\t') for line in dat]\n",
    "ent2id = {line[0]: i for i, line in enumerate(lines)}\n",
    "id2ent = {i: line[0] for i, line in enumerate(lines)}\n",
    "with open('OpenBG500/OpenBG500_relation2text.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    lines = [line.strip().split('\\t') for line in dat]\n",
    "rel2id = {line[0]: i for i, line in enumerate(lines)}\n",
    "with open('OpenBG500/OpenBG500_train.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    train = [line.strip('\\n').split('\\t') for line in dat]\n",
    "with open('OpenBG500/OpenBG500_dev.tsv', 'r', encoding='utf-8') as fp:\n",
    "    dat = fp.readlines()\n",
    "    dev = [line.strip('\\n').split('\\t') for line in dat]\n",
    "with open('OpenBG500/OpenBG500_test.tsv', 'r', encoding='utf-8') as fp:\n",
    "    test = fp.readlines()\n",
    "    test = [line.strip('\\n').split('\\t') for line in test]\n",
    "# 构建数据集\n",
    "train_dataset = TripleDataset(ent2id, rel2id, train)\n",
    "dev_dataset = TripleDataset(ent2id, rel2id, dev)\n",
    "train_data_loader = data.DataLoader(train_dataset, batch_size=train_batch_size, shuffle=True)\n",
    "dev_data_loader = data.DataLoader(dev_dataset, batch_size=dev_batch_size)\n",
    "test_dataset = TestDataset(ent2id, rel2id, test)\n",
    "test_data_loader = data.DataLoader(test_dataset, batch_size=test_batch_size)\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# 获取原始数据集的长度\n",
    "original_size = len(train_dataset)\n",
    "\n",
    "# 计算切片后的目标大小\n",
    "target_size = original_size // 100\n",
    "\n",
    "# 随机抽样得到切片后的索引\n",
    "sampled_indices = random.sample(range(original_size), target_size)\n",
    "\n",
    "# 根据抽样后的索引获取切片后的数据集\n",
    "sampled_train_data = [train_dataset[idx] for idx in sampled_indices]\n",
    "\n",
    "# 如果需要将切片后的数据集重新构建为 DataLoader\n",
    "sampled_train_data_loader = data.DataLoader(sampled_train_data, batch_size=train_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training...\n",
      "epoch:0/10, step:0/13, loss=0.6933450698852539, avg_loss=0.6933450698852539\n",
      "epoch:0/10, step:5/13, loss=0.6932802796363831, avg_loss=0.6933171550432841\n",
      "epoch:0/10, step:10/13, loss=0.693170964717865, avg_loss=0.6932828697291288\n",
      "epoch:0/10, all_loss=9.012145519256592\n",
      "epoch:1/10, step:0/13, loss=0.6927986145019531, avg_loss=0.6927986145019531\n",
      "epoch:1/10, step:5/13, loss=0.692460298538208, avg_loss=0.6928480466206869\n",
      "epoch:1/10, step:10/13, loss=0.6921534538269043, avg_loss=0.6926869208162482\n",
      "epoch:1/10, all_loss=9.004232227802277\n",
      "epoch:2/10, step:0/13, loss=0.6918404698371887, avg_loss=0.6918404698371887\n",
      "epoch:2/10, step:5/13, loss=0.6924312710762024, avg_loss=0.6915651361147562\n",
      "epoch:2/10, step:10/13, loss=0.6901764273643494, avg_loss=0.6914288564161821\n",
      "epoch:2/10, all_loss=8.986169338226318\n",
      "epoch:3/10, step:0/13, loss=0.6898820996284485, avg_loss=0.6898820996284485\n",
      "epoch:3/10, step:5/13, loss=0.6890738606452942, avg_loss=0.6897997458775839\n",
      "epoch:3/10, step:10/13, loss=0.6858291029930115, avg_loss=0.6888754855502736\n",
      "epoch:3/10, all_loss=8.951800107955933\n",
      "epoch:4/10, step:0/13, loss=0.6847022771835327, avg_loss=0.6847022771835327\n",
      "epoch:4/10, step:5/13, loss=0.6824740171432495, avg_loss=0.6839698155721029\n",
      "epoch:4/10, step:10/13, loss=0.680910050868988, avg_loss=0.6831427920948375\n",
      "epoch:4/10, all_loss=8.877695262432098\n",
      "testing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 313/313 [00:34<00:00,  9.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mrr: 3.333333370392211e-05, hit@1: 0.0, hit@3: 0.0, hit@10: 0.0002  *\n",
      "epoch:5/10, step:0/13, loss=0.6807449460029602, avg_loss=0.6807449460029602\n",
      "epoch:5/10, step:5/13, loss=0.6748770475387573, avg_loss=0.6771734257539114\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 30\u001b[0m\n\u001b[0;32m     27\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 30\u001b[0m all_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m print_frequency \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, step:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sampled_train_data_loader)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;241m.\u001b[39mitem()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, avg_loss=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mall_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m(i\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = TuckER(len(ent2id), len(rel2id), dim=embedding_dim).cuda()\n",
    "model.init()\n",
    "# model.load_state_dict(torch.load('TuckER_best.pth'))\n",
    "# 优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 训练\n",
    "print('start training...')\n",
    "for epoch in range(epochs):\n",
    "    all_loss = 0\n",
    "    for i, (local_heads, local_relations, local_tails) in enumerate(sampled_train_data_loader):\n",
    "\n",
    "        head = local_heads.cuda()\n",
    "        relation = local_relations.cuda()\n",
    "        tail = local_tails.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 计算正样本预测值\n",
    "        pred = model.forward(head, relation)\n",
    "        labels = torch.ones(len(local_heads), len(ent2id)).cuda()\n",
    "\n",
    "        # 计算损失\n",
    "        loss = model.loss(pred, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        all_loss += loss.item()\n",
    "        if i % print_frequency == 0:\n",
    "            print(\n",
    "                f\"epoch:{epoch}/{epochs}, step:{i}/{len(sampled_train_data_loader)}, loss={loss.item()}, avg_loss={all_loss / (i + 1)}\")\n",
    "    print(f\"epoch:{epoch}/{epochs}, all_loss={all_loss}\")\n",
    "\n",
    "    # 验证\n",
    "    if validation and (epoch + 1) % dev_interval == 0:\n",
    "        print('testing...')\n",
    "        improve = ''\n",
    "        mrr, hits1, hits3, hits10 = model.evaluate(dev_data_loader)\n",
    "        if mrr >= best_mrr:\n",
    "            best_mrr = mrr\n",
    "            improve = '*'\n",
    "            torch.save(model.state_dict(), 'TuckER_best.pth')\n",
    "        torch.save(model.state_dict(), 'TuckER_latest.pth')\n",
    "        print(f'mrr: {mrr}, hit@1: {hits1}, hit@3: {hits3}, hit@10: {hits10}  {improve}')\n",
    "    if not validation:\n",
    "        torch.save(model.state_dict(), 'TuckER_latest.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_all = []\n",
    "model = TuckER(len(ent2id), len(rel2id), dim=embedding_dim).cuda()\n",
    "model.load_state_dict(torch.load('TuckER_best.pth'))\n",
    "for heads, relations in tqdm.tqdm(test_data_loader):\n",
    "    # 预测的id,结果为tensor(batch_size*10)\n",
    "    predict_id = model.link_predict(heads.cuda(), relations.cuda())\n",
    "    # 结果取到cpu并转为一行的list以便迭代\n",
    "    predict_list = predict_id.cpu().numpy().reshape(1,-1).squeeze(0).tolist()\n",
    "    # id转为实体\n",
    "    predict_ent = map(lambda x: id2ent[x], predict_list)\n",
    "    # 保存结果\n",
    "    predict_all.extend(predict_ent)\n",
    "print('prediction finished !')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('TuckER_submission.tsv', 'w', encoding='utf-8') as f:\n",
    "    for i in range(len(test)):\n",
    "        # 直接writelines没有空格分隔，手工加分割符，得按提交格式来\n",
    "        list = [x + '\\t' for x in test[i]] + [x + '\\n' if i == 9 else x + '\\t' for i, x in enumerate(predict_all[i*10:i*10+10])]\n",
    "        f.writelines(list)\n",
    "print('file saved !')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
